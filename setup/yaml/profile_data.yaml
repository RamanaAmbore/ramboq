name: RAMANA RAO AMBORE
designation: APPLICATION DEVELOPMENT DIRECTOR
name suffix: 'FRM'
profile: |
  Highly accomplished FinTech professional with a strong foundation in quantitative finance, 
  evidenced by FRM certification and CFA Level 3 candidacy. Proven expertise in modernizing 
  legacy systems, optimizing high-volume trading platforms, and developing sophisticated 
  algorithmic trading solutions. Deep understanding of cloud/distributed technologies, 
  database & legacy systems, and data analytics. A results-driven professional, recipient 
  of the organization's most coveted Innovation Award, with a proven ability to deliver 
  innovative solutions and drive success within the financial markets.
experience summary:
  - Modernized a high-volume, business-critical legacy application on AWS using Java and Spark. 
    The resulting core framework reduced development and deployment turnaround times by 50 percent.
  - Developed 'Rambo,' an analytics platform that empowered software development teams to reengineer and 
    optimize software artifacts. By providing critical insights into dependencies,
    'Rambo' facilitated the identification and decommissioning of around 3000 batch jobs, significantly improving efficiency.
  - Received the Delivery Excellence Award for designing and developing a Python-based product suite to reengineer 
    legacy COBOL-VSAM applications into COBOL-DB2 applications.
  - As a Principal Software Engineer, I'm leading the development of the parsing engine for Cobra, a COBOL Research and Analysis tool. 
    This engine will track data flow, identify the origin and transformation of critical fields, and ultimately enable accurate data lineage analysis.
  - Developing an algorithmic trading platform that leverages the power of option Greeks and underlying stock prices to identify 
    and exploit fleeting market inefficiencies, enabling automated, data-driven trading decisions.
milestones:
  1993:
    milestone: Master's
    height: 0
    hover: Completed Master's degree in Computer Science, sponsored ::by Defence Research Development Organization::(DRDO)
  1996:
    milestone: Ryder
    height: 0
    hover: Worked for Ryder to optimize ROAD analytics database::as a Software Consultant
  1997:
    milestone: Dade County
    height: 0
    hover: Implemented DBS Software Year 2000 changes at 
                ::Office of Information Tech.,::Dade County Public Schools

  2002:
    milestone: Pershing
    height: 0
    hover: Automated the batch system,::optimized application support costs.
                ::Recipient of the Project of the Year Award and::commendation from CEO
  2003:
    milestone: General Motors
    height: 3.5
    hover: Led the team to extract business logic and::to document GM Dealer Information Delivery system::for leagacy modernization.
  2005:
    milestone: PGCBM
    height: 0
    hover: Completed one year executive business management program, 
                ::PGCBM from one of the most reputed business schools-XLRI
  2008:
    milestone: Fidelity
    height: 0
    hover: Started working for Fidelity. Implemented major 
                ::automation, reengineering and modernization initiatives
                ::Rambo, Cobra, VSAM-to-DB2,::legacy transmission Automation
  2020:
    milestone: FRM
    height: 0
    hover: Awarded FRM after clearing two parts of::Financial Risk Manager 
                exam conducted by GARP involving 1000 hours of learning and::preparation which
                gave a good understanding of::risk management in financial industry
  2021:
    milestone: CFA
    height: 0
    hover: Cleared two levels  of of Certified Financial::Analyst exam
                :: conducted by CFA Institute::which helped understanding trading and investment, 
                ::which gave a strong foundation in quantitative analysis
  2023:
    milestone: Innovation
    height: 0
    hover: Received the most coveted Delivery Excellence award for
                ::Innovation at NTT. One among the 40 people to receive the award 
                ::globally across the organization to receive the award.
education:
  Master's:
    icon: jk.png
    link: https://www.jkinstitute.ac.in/
    year: 1993
    duration: 2
    long label: Master's in Computer Science, J.K.Institute of Applied Physics & Tech., 1993
    hover: Master's in Computer Science::sponsored by DRDO,::2 years, 1993
  PGCBM:
    icon: xlri.png
    link: https://www.xlri.ac.in/
    year: 2005
    duration: 1
    long label: PGCBM, XLRI - School of Business, 2005
    hover: Executive business management program, XLRI::, one of the most reputed schools in India,::1 year, 2005
  Bachelor's:
    icon: nagarjuna.png
    link: https://www.nagarjunauniversity.ac.in/
    year: 1991
    duration: 3
    long label: Bachelor's in Computer Science, Nagarjuna University, 1991
    hover: Bachelor's in Computer Science, Nagarjuna University,::3 years, 1991
certifications:
  FRM:
    icon: frm.png
    link: https://www.garp.org/frm
    year: 2022
    duration: 1000
    group: domain
    long label: GARP certified Financial Risk Manager (FRM), 2022, ID# 502640
    hover: Certified Financial Risk Manager (FRM),::1000 hours of learning and preparation,::2022
  CFA:
    icon: cfa.png
    link: https://www.cfainstitute.org/
    year: 2020
    duration: 1000
    group: domain
    long label: Cleared Certified Financial Analyst (CFA) Level 1 & Level 2 exams, 2020, ID# 8243442
    hover: Cleared Certified Financial Analyst::(CFA) Level 1 & Level 2 exams,::1000 hours of learning and preparation, 2020
  DB2 DBA:
    icon: ibm_db2.png
    link: https://www.ibm.com/training/certification/ibm-certified-administrator-db2-12-for-zos-C0005400
    year: 2003
    duration: 500
    group: technical
    long label: IBM Certified DB2 DBA, 2003
    hover: IBM Certified DB2 DBA::500 hours of learning and preparation,::2003
  six sigma:
    icon: six_sigma.png
    link: https://qaiglobalinstitute.com/six-sigma-certification-courses/
    year: 2005
    duration: 200
    group: process
    long label: Six Sigma Green Belt certification, QAI, 2005
    hover: Six Sigma Green Belt certification::200 hours of learning and preparation, 2005
  PMP:
    icon: pmp.png
    link: https://www.pmi.org/certifications/project-management-pmp
    year: 2004
    duration: 200
    group: management
    long label: Project Management Professional (PMP), PMI, 2004
    hover: Project Management Professional (PMP)::200 hours of learning and preparation,:2005
  Java:
    icon: java_small.png
    link: https://www.oracle.com/in/education/training/java/
    year: 1999
    duration: 200
    group: technical
    long label: Sun Certified Java Programmer, 1999
    hover: Sun Certified Java Programmer::200 hours of learning and preparation,::1999
skills:
  python:
    icon: python.png
    level: 5
    hover: Used extensively for analytics, automation,::visualization
    duration: 8
  streamlit:
    icon: streamlit.png
    level: 4
    hover: Created analytics platform for visualizing::organizational artifacts and dependencies
    duration: 3
  dash/plotly:
    icon: dash_plotly.png
    level: 4
    hover: Created algo trading platform for derivatives
    duration: 2
  java:
    icon: java_small.png
    level: 4
    hover: Used to modernize legacy system with::critical performance objectives
    duration: 6
  spark:
    icon: apache_spark.png
    level: 4
    hover: Core technology of high volume::modernized platform for a high::volume transmission data
    duration: 3
  SpringBoot:
    icon: spring.png
    level: 3
    hover: Used to create spark batch application
    duration: 5
  COBOL:
    icon: ibm.png
    level: 5
    hover: Extensive experience writing batch, db2::and CICS applications
    duration: 25
  CICS:
    icon: ibm.png
    level: 5
    hover: Developed new online screens and modify existing screens
    duration: 20
  DB2/SQL: 
    icon: ibm_db2.png
    level: 5
    hover: Certified DB2 DBA with extensive experience::writing online and batch applications
    duration: 25
  JCL:
    icon: ibm.png
    level: 5
    hover: Extensive experience in writing batch applications
    duration: 25
  VSAM:
    icon: ibm.png
    level: 3
    hover: Used for batch and online applications
    duration: 15
  AWS:
    icon: aws.png
    level: 3
    hover: Legacy modernization platform for::EMR, S3, Lambda, EC2, etc
    duration: 5
  terracore:
    icon: terraform.png
    level: 3
    hover: Used cloud automation
    duration: 5
  jenkins:
    icon: jenkins.png
    level: 3
    duration: 5
    hover: Implemented CI/CD pipelines to::build artifacts and deploy in the code
  git:
    icon: git.png
    level: 4
    hover: Used extensively for professional and personal projects
    duration: 8
contact:
  resume:
    label: Download Resume
    icon: pdf.png
    link: resume.pdf
  mail:
    label: Ramana.Ambore@gmail.com
    icon: mail.png
    link: mailto:ramana.ambore@gmail.com
  phone:
    label: (515) 735-1486
    icon: phone.png
    link: tel:+15157351486
  loc:
    label: West Des Moines, Iowa
    icon: loc.png
    link: https://www.google.com/maps/@41.5639989,-93.8009926,13z

social:
  ramana_portal:
    label: ramanaambore.me
    icon: rambo.png
    link: https://ramanaambore.me
  linkedin:
    label: linkedin.com/in/ambore
    icon: linkedin.png
    link: https://www.linkedin.com/in/ramanaambore
  github:
    label: github.com/RamanaAmbore
    icon: github.png
    link: https://github.com/RamanaAmbore

  medium:
    label: medium.com/@rrambo
    icon: medium.png
    link: https://medium.com/@rrambo
portfolio:
  profile builder:
    icon: rambo.png
    link: https://medium.com/@rrambo/streamlit-profile-builder-a-simple-way-to-create-interactive-online-profiles-3cb0b0f6c8bb
    github: https://github.com/RamanaAmbore/stremlite_profile
    image: portfolio1.png
    summary: |
      A dynamic, professional profile website built with Streamlit. This app is designed to showcase your profile, skills, experience, 
      and portfolio in a visually appealing and interactive format. It leverages Streamlit's features and custom JavaScript for smooth scrolling navigation.
    technology: Python,  Pandas, Streamlit
    additional information: |
      **Project: Streamlit Powered Profile Builder**
      
      **Description:** A dynamic Streamlit-powered profile builder showcasing skills, experience, and portfolio with interactive features and smooth scrolling navigation.
      
      **Main Highlights:**
      
      * **Smooth Scrolling:** Navigate seamlessly to different sections of the profile using JavaScript.
      * **Dynamic Sidebar:** A sidebar menu for quick navigation between sections.
      * **Comprehensive Sections:**
          * Profile: Display your name, tagline, and basic info.
          * Social Links: Provide easy access to your social media profiles.
          * Experience Summary: Summarize your career achievements.
          * Skills: Showcase your technical and soft skills.
          * Projects and Portfolio: Highlight your key projects and accomplishments.
          * Education: Add details of your academic qualifications.
          * Certifications: Showcase professional certifications.
          * Hobbies: Add a personal touch with your interests.
      
      **Challenges:**
      
      * **Dynamic Content Integration:** Smoothly integrating and updating project descriptions, skills, and work experience.
      * **Interactive Elements:** Implementing smooth scrolling and engaging interactive elements.
      * **Cross-browser Compatibility:** Ensuring consistent performance across different browsers.
      * **Responsive Design:** Adapting the website to various screen sizes.
      * **Performance Optimization:** Minimizing loading times and ensuring a smooth user experience.
      * **Accessibility:** Designing the website for users with disabilities.
      * **Deployment and Maintenance:** Deploying and maintaining the app effectively.
      
      **Tech Stack:**
      
      * **Streamlit:** For building the UI.
      * **Python:** For logic and backend handling.
      * **JavaScript:** For enabling smooth scrolling.
      * **Pillow:** For image processing.
      * **HTML & CSS:** For custom styling.
  RRambo Trader - Algo Trading for Derivatives:
    icon: trader.png
    link: https://medium.com/@rrambo
    github: https://github.com/RamanaAmbore
    image: portfolio2.png
    summary: |
      This project aims to develop a sophisticated algorithmic trading platform designed to identify and capitalize on short-term market inefficiencies 
      in option prices. Leveraging the power of Python, Dash, and Plotly, the platform will utilize option Greeks, underlying stock prices, and advanced 
      statistical models to generate robust trading signals and execute trades automatically.
    technology: Python,  Pandas, Dash, Plotly
    additional information: |
      **Project: RRambo Trader - Algo Trading for Derivatives**
  
      **Description:** This project aims to develop a sophisticated algorithmic trading platform designed to identify and capitalize on short-term market inefficiencies in option prices.
  
      **Main Highlights:**
      
      * **Robust Backtesting Framework:** Develop and implement a robust backtesting framework to rigorously evaluate trading strategies on historical market data.
      * **Risk Management:** Incorporate risk management rules, such as position sizing, stop-loss orders, and portfolio diversification, to minimize potential losses.
      * **Automated Trading:** Integrate with a brokerage API to enable automated order execution and trade management.
      * **Continuous Improvement:** Conduct ongoing research and development to refine trading strategies, incorporate new market data sources, and adapt to evolving market conditions.
      * **User-Friendly Interface:** Develop a comprehensive documentation and user manual to guide users in utilizing the platform effectively.
  
      **Challenges:**
      
      * **Market Volatility:** Accurately predict and adapt to rapidly changing market conditions and volatility.
      * **Data Management:** Efficiently collect, clean, and manage high-frequency market data.
      * **Model Development:** Develop, backtest, and validate robust trading models.
      * **Risk Mitigation:** Implement effective risk management strategies.
      * **Real-time Data Processing:** Process and analyze market data in real-time.
      * **Automated Execution:** Implement a reliable and efficient system for automated trading.
      * **Regulatory Compliance:** Adhere to relevant regulations for algorithmic trading.
      * **Scalability and Performance:** Ensure the platform can handle high volumes of data and execute trades efficiently.
      * **Continuous Optimization:** Continuously backtest and optimize trading strategies.
      * **Ethical Considerations:** Address ethical considerations related to algorithmic trading.
  
      **Tech Stack:**
      
      * **Programming Languages:** Python
      * **Data Science Libraries:** Pandas, NumPy, Scikit-learn, Statsmodels
      * **Data Visualization:** Plotly, Dash
      * **Cloud Computing (Optional):** AWS, Google Cloud, Azure
      * **Brokerage API Integration:** Interactive Brokers API, Alpaca API, or other relevant APIs
projects:
  NTT DATA:
    short label: NTT
    long label: NTT DATA Global Delivery Services Ltd., Application Development Director, Apr 2003 - Till Date
    hover: NTT DATA Global Delivery Services,::Application Development Director,::21 years, Apr 2003 - Till Date
    icon: ntt.png
    duration: 21
    link: https://www.nttdata.com/global/en/
    clients:
      Fidelity:
        1. Cloud based Transmission architecture, Design, Development:
          role: Principle Software Engineer
          start: Apr 2003
          end: Till Date
          duration: 21 years
          technology: Java, Springboot, Spark, Oracle, AWS, Python, Pandas, GIT, Terraform, Jenkins,
          summary: |
            Led the modernization of a high-volume, business-critical legacy application on AWS, leveraging Java, Spark, and a developed core framework. This significantly improved development and deployment speed, achieving a 50% reduction in turnaround times. Key responsibilities included designing high-volume transmission files, implementing data parsing and validation, and ensuring data consistency with mainframe systems.
          additional information: |
            **Project: Modernization of High-Volume Legacy Application**
            
            **Description:** Modernized a high-volume, business-critical legacy application on AWS using Java and Spark. This involved:
            
            * **Core Framework Development:** Created a core framework that reduced development and deployment turnaround times by 50 percent.
            * **High-Volume Data Processing:** Developed high-volume transmission files generated from the new system using complex SQL queries.
            * **Data Transformation:** Parsed SQL output to match the exact format of the legacy transmission files.
            * **Comprehensive Solution Implementation:** Implemented the entire solution as a reusable core framework for future projects.
            * **Data Integrity:** Ensured data accuracy by rigorously validating transmission files against the format of legacy mainframe transmissions.
            * **Enhanced Reliability:** Developed mechanisms within the core framework to recreate transmissions for specific clients within defined Service Level Agreements (SLAs) in case of failures.
            * **Technology Stack:** Leveraged technologies such as Spring Boot, Spark, EMR, and AWS services for efficient data processing and deployment.
            * **Legacy System Analysis:** Analyzed existing mainframe processes utilizing tools like REXX to understand and replicate critical functionalities.
            
            This project successfully modernized a critical business application, improving efficiency, agility, and reliability while leveraging the power of cloud technologies.
        2. Rambo - Rapid Analysis of Mainframe Batch Objects:
          role: Principle Software Engineer
          start: Apr 2003
          end: Till Date
          technology: Paython, Pandas, Postgres, Networkx, Pyvis, Streamlit, GIT, Jenkins
          summary: |
            I led the development of 'Rambo,' an analytics platform that empowered software development teams to reengineer and optimize software artifacts by providing critical insights into dependencies. My responsibilities included identifying metadata sources on software artifacts, organizing and storing the data in a database, creating networkx graph structures with nodes and edges, parsing the data for various use cases with complex SQLs, and presenting data flow diagrams in a visual manner using Python, PyVis, SQL, REXX, and Streamlit.
          additional information: |
            **Responsibilities:**
            
            * **Platform Development:** Developed 'Rambo,' an analytics platform empowering software development teams to reengineer and optimize software artifacts by providing critical insights into dependencies.
            * **Metadata Management:** Identified metadata sources on software artifacts, organized, and saved the data in the database.
            * **Graph Structure:** Created networkx graph structures with nodes and edges, parsing the data for various use cases with complex SQLs.
            * **Data Visualization:** Presented data flow diagrams visually using Python, PyVis, SQL, REXX, and Streamlit.
        3. Cobra - COBOL Research & Analysis:
          role: Principle Software Engineer
          start: Apr 2003
          end: Till Date
          technology: Python, Pandas, Regex, Neworkx, Postgres, Pyvis, Streamlit, GIT, Jenkins
          summary: |
            This project centers around the development of "Cobra," a comprehensive ecosystem specifically designed to address the challenges of modernizing and enhancing legacy COBOL applications. Recognizing the critical role these systems play in many organizations, Cobra aims to equip businesses with the tools and technologies necessary to effectively understand, analyze, and ultimately modernize their existing COBOL codebases. This multifaceted ecosystem will encompass a suite of integrated tools and functionalities, empowering organizations to gain deeper insights into their COBOL applications and explore various modernization pathways.
          additional information: |
            **Project: Cobra - COBOL Research & Analysis**
            
            **Description:** This project involves the development of a comprehensive ecosystem, "Cobra," designed to modernize and enhance legacy COBOL applications. Cobra will provide a suite of tools and technologies to help organizations understand, analyze, and modernize their existing COBOL codebases.
            
            **Key Components:**
            
            * **High-Performance Parsing Engine:** 
                * Develops a robust parser to accurately analyze COBOL code, including lexical and syntax analysis, handling various dialects, and constructing an Abstract Syntax Tree (AST).
                * Extracts key information such as data flow, control flow, and data dependencies.
            * **Data Lineage Analysis:** 
                * Tracks the origin and transformation of critical data elements throughout the application.
                * Provides visual representations of data flows using interactive diagrams and visualizations.
            * **Code Analysis and Optimization Tools:** 
                * Identifies potential areas for code improvement, such as performance bottlenecks, code redundancy, and potential security vulnerabilities.
                * Suggests refactoring opportunities to improve code maintainability and readability.
            * **Modernization Pathways:** 
                * Explores and evaluates potential modernization strategies, such as:
                    * Code refactoring and optimization
                    * Partial or full code translation to modern languages
                    * Integration with modern technologies and architectures
            * **User Interface and Documentation:** 
                * Develops a user-friendly interface for interacting with the Cobra ecosystem, including data visualization tools and interactive dashboards. 
                * Provides comprehensive documentation and tutorials to guide users in effectively utilizing Cobra.
            
            **Challenges:**
            
            * **Handling the Complexity of COBOL:** Dealing with the intricacies of the COBOL language, including various dialects, complex data structures, and decades of accumulated code.
            * **Ensuring Data Accuracy and Consistency:** Maintaining the accuracy and consistency of data extracted from the COBOL codebase.
            * **Developing Scalable and Performant Solutions:** Creating tools that can efficiently handle large-scale COBOL applications and provide timely results.
            * **Addressing the Evolving Landscape of COBOL:** Adapting to new COBOL standards, emerging technologies, and evolving business requirements.
            * **Building a User-Friendly Experience:** Designing an intuitive and user-friendly interface for interacting with the Cobra ecosystem.
            
            **Key Skills:**
            
            * **Strong proficiency in Python programming**
            * **Deep understanding of COBOL language syntax and semantics**
            * **Experience with parsing techniques (lexical analysis, syntax analysis, AST construction)**
            * **Familiarity with parsing tools and libraries (e.g., PLY, ANTLR)**
            * **Data analysis and visualization skills (e.g., using libraries like Pandas, NumPy, Matplotlib, Plotly)**
            * **Software engineering best practices (design patterns, testing, code reviews)**
            
            This project represents a significant undertaking in modernizing legacy COBOL systems. By addressing the challenges and leveraging the power of modern technologies, Cobra - COBOL Research & Analysis can empower organizations to unlock the value of their existing COBOL investments while preparing them for the future.
        4. Legacy Automation:
          role: Principle Software Engineer
          start: Apr 2003
          end: Till Date
          technology: MVS, COBOL, CICS, JCL, DB2, VSAM, Assembler
          summary: |
            The project focused on modernizing legacy transmission job automation processes within our organization. The primary objective was to replace static job schedules with dynamic job submissions triggered by events or data changes within the CICS environment.
          additional information: |
       
            **Project: Legacy Transmission Job Automation**
            
            **Description:** This project focused on modernizing legacy transmission job automation processes within our organization. The primary objective was to replace static job schedules with dynamic job submissions triggered by events or data changes within the CICS environment. 
            
            **Key Achievements:**
            
            * **Developed and implemented a framework for dynamic job submission:** 
                * Leveraged CICS internal readers to monitor specific transactions, messages, or data changes within the CICS environment.
                * Developed custom CICS programs (written in COBOL) to process these events and trigger appropriate job submissions.
                * Integrated with the Job Control Language (JCL) to dynamically generate and submit jobs to the mainframe batch environment.
            * **Enhanced Job Scheduling Flexibility:** 
                * Replaced static job schedules with dynamic triggers, enabling more responsive and event-driven job execution.
                * Improved resource utilization by executing jobs only when necessary, reducing unnecessary processing overhead.
            * **Improved Operational Efficiency:** 
                * Streamlined job scheduling processes, reducing manual intervention and the risk of human error.
                * Improved overall system responsiveness and reduced job execution delays. 
            * **Technology Stack:**
                * **COBOL:** Primary language for developing CICS programs and processing business logic.
                * **CICS:** Provided the runtime environment for CICS programs and facilitated event-driven processing.
                * **JCL:** Used for defining and submitting batch jobs to the mainframe.
                * **DB2:** Utilized for data storage and retrieval.
                * **REXX:** (Optional) Potentially used for scripting and automation tasks within the CICS environment.
            
            **Benefits:**
            
            * **Increased agility and responsiveness:** Enabled the organization to react more quickly to changing business needs and events.
            * **Improved operational efficiency:** Reduced manual intervention, minimized errors, and improved resource utilization.
            * **Enhanced system stability:** Reduced the risk of missed jobs or scheduling conflicts.
            * **Modernized legacy systems:** Brought legacy job scheduling processes in line with modern IT practices.
      General Motors:
        General Motors Dealer Information Database (GMDID) Re-engineering Project:
          summary: |
            As a lead, I prepared program logic documents and functional specifications, and created project proposals for GMDID/DNPS systems. I developed REXX tools to extract information for creating Functional Specifications. I conducted impact analysis and documented data access issues. I assisted in data retrieval, batch processes, and database issues for Siebel integration. I also created Functional Specifications and project proposals, and managed project planning, scheduling, and monitoring.


          role: Project Leader
          start: May 2002
          end: Apr 2003
          technology: MVS, REXX, JCL, COBOL, IMS, DB2, CICS
          additional information: |
            **Redesign and Enhancement of Weekly Batch Cycle with Offshore Support:**

            * **Leadership and Documentation:** Led the teams in preparing program logic documents and functional specifications using 2440 COBOL/IMS/DB2 programs, 765 copybooks, and 180 JCLs, and creating project proposals for GMDID/DNPS systems.
            * **REXX Tools Creation:** Developed REXX tools to extract information related to program logic, screen layout, and dataset & database access for creating the Functional Specification.
            * **Impact Analysis:** Conducted impact analysis on non-GMDID/DNPS applications that access the GMDID/DNPS database. Researched and documented issues related to data access to non-GMDID/DNPS databases.
            * **Siebel Team Support:** Assisted the Siebel team in understanding data retrieval, updates, and user access rights in mainframe screens, and in creating the corresponding Siebel screens.
            * **Batch Process Understanding:** Supported the Siebel team in understanding the mainframe batch process and creating the corresponding batch and reporting functionality in the Siebel system.
            * **Database-related Issues:** Collaborated with the Siebel team to address database-related issues such as database schema, data migration, and database connectivity for other applications.
            * **Functional Specification and Project Proposal:** Actively involved in creating Functional Specifications (FS) and project proposals for the re-engineering project. Responsible for planning, scheduling, and monitoring project activities according to the project plan.

  iNautix Technologies:
    short label: iNautix
    icon: inautix.png
    duration: 1
    link: https://www.bny.com/pershing/us/en.html
    long label: iNautix Technologies India Pvt, Project Leader, May 2002 - Apr 2003
    hover: iNautix Technologies, Project Leader,::1 year, May 2002 - Apr 2003
    clients:
      Pershing:
        Meta Data Services Group:
          role: Project Leader
          start: May 2002
          end: Apr 2003
          technology: MVS, REXX, JCL, COBOL, DB2, CA-7, Endevor, Platinum/DataShopper, Java, Java Script, DB2 Connect
          summary: |
            Meta Data Services group of  iNautix provides offshore support to the parent company – Pershing to capture, 
            store and present the Meta Data related to mainframe source code using Platinum Repository and DataShopper
          additional information: |
            **Redesign and Enhancement of Weekly Batch Cycle with Offshore Support:**
            
            * **Transition Planning:** Created transition plans and coordinated with the delivery manager to recruit and train resources for the project. Worked with contractors to conduct knowledge transfer sessions for offshore team members.
            * **Estimation and Planning:** Performed estimation, planning, and scheduling for tasks involved in capturing, storing, and presenting metadata using Platinum/Repository. Identified inefficiencies in the weekly batch process and redesigned the weekly batch cycle.
            * **Code Organization:** Organized and consolidated the source code related to metadata on the mainframe. Submitted a proposal for an internet-based ‘DBA Online Request Processing System’ to submit, track, and fulfill database-related requests.
            * **Technical Support:** Assisted team members in resolving various technical issues.
            * **REXX Tools Implementation:** Redesigned the weekly batch cycle using REXX tools to submit jobs recursively and ran it successfully through CA-7. Identified and resolved various quality issues with the metadata. Implemented enhancements requested by business users.
            * **Documentation and Automation:** Created standard templates to document various project activities. Implemented processes to identify and fix issues in repository maintenance. Assisted the team in creating tools using REXX and MS Access to automate and document repetitive tasks. Participated in CMM Level 3 process-related activities.
            * **Training Sessions:** Conducted training sessions for team members on core skills—REXX and Java, DB2 Database Administration—required for the project. Created training material on Repository/Data Shopper and conducted training sessions for new associates. Helped associates acquire DB2 Database administration technology through IBM certification.
            * **Recognition:** Received the Star award within three months of joining the company for contributions to the project.

  Modis Consulting:
    short label: Moids
    long label: Modis Consulting, Software Consultant, Nov 1997 Apr 2002
    hover: Modis Consulting, Software Consultant,::5 years, Nov 1997 Apr 2002
    icon: modis.png
    link: https://www.akkodis.com/
    duration: 5
    clients:
      Dade Country Schools:
        DBS Financials:
          role: Software Consultant
          start: Apr 2002
          end: Nov 1997
          technology: MVS, COBOL, EasyTrieve, JCL, REXX, CICS, VSAM, FileAid, Xpediter, DCI
          summary: |
            The project is to install, customize and implement DBS E-series packages for Dade County Public Schools. 
              The project also involved developing Internal Funds systems and integrating it with DBS E-Series packages
          additional information: |
            **Consultant for DBS E-Series Financial System Implementation and Internal Funds System Development:**
            
            * **Project Implementation:** Assisted the project manager in implementing the DBS E-series financial system and developing the Internal Funds System.
            * **Framework Creation:** Actively participated in creating the framework for implementing in-house enhancements and service requests.
            * **Documentation:** Collaborated with team members to create documentation for screen and report layout changes.
            * **Technical Support:** Helped team members resolve technical issues, create test cases, and integrate other in-house systems with the new release.
            * **Integration Troubleshooting:** Involved in troubleshooting various integration issues. Consulted with the vendor to apply bug fixes and resolve integration issues with budgetary and financial controller systems.
            * **System Rewrite:** Rewrote the Internal Funds online system using Dun & Bradstreet Software’s DCI.
            * **User Manual & Training:** Worked with the documentation team to update the user manual and conducted training sessions for clients to use the new system.
            * **Level 2 Support:** Provided level 2 support for the new release.

  Mastech Inc:
    label: Mastech
    icon: igate.png
    duration: 1
    link: https://www.capgemini.com/
    long label: Matech Systems, Software Consultant, Nov 1996 - Nov 1997
    hover: Matech Systems, Software Consultant,::1 year, Nov 1996 - Nov 1997
    clients:
      Ryder Inc:
        Ryder On-line Analytical Decision (ROAD) System:
          role: Senior Software Consultant
          start: Nov 1996
          end: Oct 1997
          technology: MVS, COBOL, JCL, REXX, CICS, DB2, VSAM, DCI, IE, Script, Xpediter, SPUFI, FileAid, QMF
          summary: |
            Ryder On-line Analytical Decision (ROAD) Support System helps in calculating the average number repair hours at 
            the repair shops. The system generates productivity reports and helps in estimating the manpower requirements 
            at various Ryder repair shops
          additional information: |
            **Consultant for Analysis, Design, and Implementation of ROAD:**
            
            * **Client Requirements:** Consulted and reviewed client requirements for the new system. Developed new programs and JCLs, and created installation and maintenance procedures for system upkeep.
            * **System Installation & Maintenance:** Collaborated with the maintenance team to install the system and participated in initial system maintenance.
            * **Data Integrity:** Worked with the client to resolve various data integrity issues in the database.
            * **VIN Subsystem Development:** Developed a VIN subsystem to report database inconsistencies and correct them using online screens.

  Dun & Bradstreet Satyam Software:
    short label: DBSS
    long label: Dun & Bradstreet Satyam Software, Software Associate, Oct 1994 - Oct 1996
    hover: Dun & Bradstreet Satyam Software,::Software Associate,::2 years,Oct 1994 - Oct 1996
    icon: cts.png
    duration: 2
    link: https://www.cognizant.com/in/en
    clients:
      Dun & Bradstreet:
        DBS E-series Year 2000 Conversion Projects:
          role: Software Engineer
          start: Oct 1994
          end: Oct 1996
          technology: MVS, COBOL, JCL, REXX, CICS, VSAM, DCI, IE, Script, Xpediter, FileAid
          summary: |
            Dun & Bradstreet E-series Government is a suit of mainframe financial packages used by Fortune 500 companies 
            with DB2 as the database management system.  The project is to implement the new release of the package that 
            is year 2000 compliant. Worked as a team member to build Design Document for Year 2000 changes and product enhancements for the new 
            release of Purchasing System (PS) Module of DBS E-series products.
          additional information: |
            **Collaborative Implementation of Year 2000 Enhancements for Dun & Bradstreet Software:**
  
            * **Year 2000 Compliance:** As a key team member, I contributed to implementing and reviewing Year 2000 (Y2K) changes and product enhancements for the DB2 version of Purchasing and Common Components Modules.
            * **Scrolling Functionality:** Designed and implemented scrolling functionality in Inquiry Screens.
            * **Variable Storage Issue:** Resolved issues related to variable storage in CICS screens; performed bug fixes and client-requested enhancements.
            * **Automation with REXX:** Developed REXX tools to automate Y2K changes, enhancing efficiency.
            * **Peer Reviews & Quality Assurance:** Participated in peer reviews as part of the quality assurance process. Worked with the QA team to create quality procedures for ISO 9000 certification.
            * **Recognition:** Awarded 'Project of the Year' for exceptional performance and contributions.

interests:
  summary: |
    The hobby project integrated a Raspberry Pi onto a drone to enhance its capabilities beyond basic flight. Challenges include minimizing weight and power consumption, ensuring reliable communication, and developing real-time processing algorithms while addressing the harsh environment. This project provides a rewarding learning experience in drone technology and embedded systems.
  additional information: |
    **Project: Raspberry Pi Drone Enhancement**
    
    **Description:** This hobby project successfully integrated a Raspberry Pi onto a drone platform, significantly enhancing its capabilities beyond basic flight control. 
    
    **Key Achievements:**
    
    * **Real-time Image Processing:** Implemented real-time object detection and tracking using OpenCV, enabling the drone to autonomously follow moving targets or identify areas of interest.
    * **Improved Flight Stability:** Utilized sensor data from the Pi to refine flight control algorithms, resulting in smoother and more stable flight performance.
    * **Autonomous Mission Execution:** Developed and successfully tested autonomous flight modes, including "follow me," waypoint navigation, and precision landings.
    
    **Technical Considerations:**
    
    * **Weight Optimization:** Minimized the weight and size of the Raspberry Pi and its components through careful selection and custom enclosures.
    * **Power Efficiency:** Implemented power-saving techniques, including dynamic clock scaling and sleep modes, to optimize battery life.
    * **Robust Communication:** Established a reliable Wi-Fi communication link between the Pi and a ground station, ensuring stable data transmission.
    * **Environmental Hardening:** Implemented vibration isolation and thermal management solutions to protect the Pi from the drone's operating environment.
    
    **Key Learnings:**
    
    * This project provided valuable hands-on experience in drone technology, embedded systems, and computer vision.
    * The challenges of weight, power consumption, and environmental factors were successfully addressed through careful design and implementation.
    * The project demonstrates the potential of integrating small computers like the Raspberry Pi to unlock new levels of functionality in drone applications.
    
    This project serves as a foundation for future explorations in advanced drone technologies, such as artificial intelligence, machine learning, and swarm robotics.
